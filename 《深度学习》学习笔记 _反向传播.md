# 《深度学习》学习笔记 _反向传播

​		反向传播是深度学习衡量某一个参数变化对最终输出结果的影响，其中主要的原理就是链式法则，也就是高等数学里面隐函数函数求导的原则，当需要对一个隐函数进行求导的时候，此时是从外向内逐层求导。其中要将除了求导元素以外的变量看成是一个常数，。对于加法节点，其反向传播的影响和上一节点不变，对于乘法节点，反向传播的影响是上一节点的影响因子乘以正向传播的因子，须注意这个正向传播的因子不是本求导变量，而是其余变量的正向传播因子。

乘法层代码：

```python
class MulLayer:
    def __init__(self):
    	self.x = None
        self.y = None
    def forward(self,x,y):
        self.x = x
        self.y = y
        out = x*y
        return out
    def backward(self,dout):
        dx = dout * self.y
        dy = dout * self.x
        
        return dx,dy
               
```

加法层代码：

```python
class AddLaryer:
    def __init__(self):
        pass
    def forward(self,x,y):
        out = x + y
        return out
    def backward(self,x,y):
        dx = dout * 1
        dy = dout * 1
        return dx,dy
```

神经网络中激活层的实现：

ReLU层：
$$
\begin{equation}
y = \left\{
\begin{aligned}
	x \quad (x>0) \\ 
	0 \quad (x\le0)
\end{aligned}
\right
.
\end{equation}
$$
求导后：
$$
\begin{equation}
\frac{\partial{y}}{\partial{x}} = \left\{
\begin{aligned}
1 \quad (x>0) \\
0 \quad (x\le{0})

\end{aligned}


\right
.
\end{equation}
$$

```python
class Relu:
    def __init__(self):
        self.mask = None
    def forward(self,x):
        self.mask = (x<=0)
        out = x.copy()
        out[self.mask] = 0
        return out
    def backward(self,dot):
        dout[self.mask] = 0
        dx = dout
        return dx
    
```

Sigmoid层
$$
y = \frac{1}{1+exp(-x)}
$$

```python
class Sigmoid:
    def __init__(self):
        self.out = None
    def forward(self,x):
        out = 1/(1+np.exp(-x))
        self.out = out
        return out
    def backward(self,dout):
        dx = dout * (1.0-self.out)*self.out
        return dx
```

