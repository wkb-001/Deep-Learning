# 《深度学习》学习笔记 _梯度法 

​		在神经网络中，主要的任务就是寻找最优参数组合。这里所说的最优参数其实就是指是的损失函数取得最小的参数，但是在神经网络中，损失函数往往很复杂，参数空间庞大，找不到哪里才能取到最优。此时梯度法就派生了用场，在梯度法中，函数的取值从当前位置沿着梯度前进一步，然后在新的位置重新计算新的梯度，重复进行，不断沿着梯度方向前进，使得函数值逐渐减小。这个过程就是梯度法（gradient method）。梯度法是机器学习中最优化的常用方法，特别是在神经网络中经常被使用到。沿着梯度方向虽然不能保证一直指向最小值，但是沿着梯度方向能够最大限度的减少函数值，所以在寻找函数最小值的位置任务中，要以梯度为线索进行查找。  

计算梯度的python代码：

```python
def numercal_gradient(f,x):
    h = 1e-4
    grad = np.zeros_like(x)
    for i in range(x.size):
        tmp_val = x[i]
        #计算f(x+h)
        x[i] = tmp_val + h
        fxh1 = f(x)
        #计算f(x-h)
        x[i] = tmp_val - h
        fxh2 = f(x)
        
        grad = (fxh1 - fxh2)/2
        x[i] = tmp_val
 return grad
```

